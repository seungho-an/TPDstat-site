[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "assignments/01_hw_data_viz.html",
    "href": "assignments/01_hw_data_viz.html",
    "title": "Problem Set 1: Data Visualization",
    "section": "",
    "text": "In this problem set, you will get your bearings on how to produce an Rmarkdown report and how to produce data visualizations using ggplot. The data we will use is the Gapminder dataset, which gives some economic and demographic information about countries over time. The variables in this data are described below.\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ncountry\nname of the country\n\n\ncontinent\nname of the country’s continent\n\n\nyear\nyear of the measurement, ranging from 1952 to 2007 in 5-year increments\n\n\nlifeExp\nlife expectancy at birth, in years\n\n\npop\npopulation\n\n\ngdpPercap\nGDP per capita (US dollars, inflation-adjusted)\n\n\n\n\n\n\n\nNOTE: In this template, the default settings for chunks is to show both R codes and the output of the chunks (echo = TRUE); if you do not want to show the R codes, you can change it to echo = FALSE, which can generate a clean look and highlight the output rather than the source code. However, for the purpose of grading (or evaluation), let’s keep the current option as it is."
  },
  {
    "objectID": "assignments/01_hw_data_viz.html#question-1",
    "href": "assignments/01_hw_data_viz.html#question-1",
    "title": "Problem Set 1: Data Visualization",
    "section": "Question 1",
    "text": "Question 1\nMake sure that you load the gapminder and tidyverse packages in the setup chunk (right after the header). For this question, use the glimpse function to show basic information about the gapminder dataset. In the main text (that is, outside of a code chunk), tell us how many rows and columns there are in the data set and which of the variables are factors."
  },
  {
    "objectID": "assignments/01_hw_data_viz.html#question-2",
    "href": "assignments/01_hw_data_viz.html#question-2",
    "title": "Problem Set 1: Data Visualization",
    "section": "Question 2",
    "text": "Question 2\nLet’s investigate how life expectancy varies across the continents. Using ggplot, we want you to recreate the following figure:\n\nThese are boxplots of the distribution of life expectancy in each continent. Please make sure that you include the labels as shown in this figure. Your code should look like this:\n\nplot_q2 <- ggplot(<arguments>) +\n  geom_<type>(<arguments>) +\n  ...\n\nplot_q2"
  },
  {
    "objectID": "assignments/01_hw_data_viz.html#question-3",
    "href": "assignments/01_hw_data_viz.html#question-3",
    "title": "Problem Set 1: Data Visualization",
    "section": "Question 3",
    "text": "Question 3\nLooking at the previous plot, which continent has the highest median life expectancy? Which part of the boxplot can we determine this from?"
  },
  {
    "objectID": "assignments/01_hw_data_viz.html#question-4",
    "href": "assignments/01_hw_data_viz.html#question-4",
    "title": "Problem Set 1: Data Visualization",
    "section": "Question 4",
    "text": "Question 4\nThe previous boxplot groups all the years together into one boxplot, but what if we want to understand how life expectancy is changing over time? Next, we will recreate the following plot:\n\nThe plot shows each country’s life expectancy trajectory over time, broken out by continent with smoothed average lines overlayed for each continent. To get started, we’ll give you a few clues about what we’ve done here:\n\nThe lines for each country use the color \"gray70\".\nThe size of the smoothed line is 1.1 and the method used is the loess smoother. We also have turned off the standard errors.\nMake sure that the facets are all on one row. Look at the facet_wrap documentation if need help with this.\nMake sure that the labels are correctly specified.\nUse the chunk options fig.width = 11 and fig.height = 4 to shrink the font size so the year labels will not overlap (this is for Rmarkdown).\n\nFinally, assign the output of your ggplot call to plot_q4 and then evaluate plot_q4 (similarly to what you did in Question 2)."
  },
  {
    "objectID": "assignments/02_hw_data_wrangling.html",
    "href": "assignments/02_hw_data_wrangling.html",
    "title": "Problem Set 2: Data Wrangling",
    "section": "",
    "text": "Note: The due is extended to March 17th since the assignment is posted a day late.\nDownload the data below for this exercise!"
  },
  {
    "objectID": "assignments/02_hw_data_wrangling.html#background",
    "href": "assignments/02_hw_data_wrangling.html#background",
    "title": "Problem Set 2: Data Wrangling",
    "section": "Background",
    "text": "Background\nAutomobile manufacturers are transitioning into producing more Electric Vehicles (EVs). EU lawmakers even passed a law that bans the sale of new petrol and diesel cars in the European Union from 2035 to speed up the switch to EVs. EVs are becoming more popular, and more EVs options are available nowadays.\nIn this problem set, you will explore a dataset that has information on EV car registration in Washington state from 2011 to 2023. The variables in this data are described below.\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nvin\nVIN number\n\n\ncounty\ncounty name\n\n\ncity\ncity name\n\n\npostalcode\nPostal code\n\n\nmodelyear\nYear where the car is intended to be launched\n\n\nmake\nCar manufacturer\n\n\nmodel\nCar model\n\n\nevtype\nEV car type (EV or hybrid)\n\n\nrange\nMaximum driving range with full charge"
  },
  {
    "objectID": "assignments/02_hw_data_wrangling.html#question-1",
    "href": "assignments/02_hw_data_wrangling.html#question-1",
    "title": "Problem Set 2: Data Wrangling",
    "section": "Question 1",
    "text": "Question 1\nLoad the data using the read_csv function (loaded together with the tidyverse package) and save it as ev_wa; assigning this will make it automatically tibble. How many electric vehicles are registered in WA since 2011 (modelyear)?\nAlso, use dplyr functions to create a table with the total number of registered BEVs and PHEVs by model year, and employ the function knitr::kable() on this table to have a nicely formatted table produced in the knitted output (also with nice labels)."
  },
  {
    "objectID": "assignments/02_hw_data_wrangling.html#question-2",
    "href": "assignments/02_hw_data_wrangling.html#question-2",
    "title": "Problem Set 2: Data Wrangling",
    "section": "Question 2",
    "text": "Question 2\nCreate a bar plot to present which county has the highest EV registered numbers over the years."
  },
  {
    "objectID": "assignments/02_hw_data_wrangling.html#question-3",
    "href": "assignments/02_hw_data_wrangling.html#question-3",
    "title": "Problem Set 2: Data Wrangling",
    "section": "Question 3",
    "text": "Question 3\nCreate a histogram of ranges for BEVs that are manufactured after 2017 (modelyear>2017) only. Once you create the histogram, in the text, describe the shape of the histogram and tell the reader if most BEVs have ranges greater than 200 miles. Also, which range seems to have the highest count in the histogram looking at the plot? Is the plot skewed to the left or right (or evenly distributed)?"
  },
  {
    "objectID": "assignments/02_hw_data_wrangling.html#question-4",
    "href": "assignments/02_hw_data_wrangling.html#question-4",
    "title": "Problem Set 2: Data Wrangling",
    "section": "Question 4",
    "text": "Question 4\nNow, you are in the BEV market and were wondering which BEV model has the longest range. Create a bar plot using the ggplot package that shows the maximum value of the range for each car model. On x-axis and y=axis, it should show the maximum range and the car model, respectively. The final figure should reorder car models by its range; the highest range car model should appear on top. Furthermore, let’s color the bars by automakers (you can pick your own color)!"
  },
  {
    "objectID": "assignments/02_hw_data_wrangling.html#question-5",
    "href": "assignments/02_hw_data_wrangling.html#question-5",
    "title": "Problem Set 2: Data Wrangling",
    "section": "Question 5",
    "text": "Question 5\nWe are going to create the difference in the car registration by model year between US and non-US brands. US brands in the dataset are Tesla, Chevrolet, Chrysler, Cadillac, and Ford (based on my quick Google search). Once you create the new vector, let’s create a bar plot to see the trend by model year. What are the trends? Are US brands being registered more and more?\nTips: Once you count the total number of registration for the US brand and non-US brand, you can use spread() function from the tidyr package; we have not covered the spread() function in the class. If you want to follow this route, you can write out as spread(usauto, n) (here n is the total number of registration), for example. This can help calculating the difference between US and non-US brand registrations much easier."
  },
  {
    "objectID": "assignments/03_hw_causality.html",
    "href": "assignments/03_hw_causality.html",
    "title": "Problem Set 3: Are Black Legislators Willing to Help African Americans Without Political Incentives?",
    "section": "",
    "text": "Data: aa_rep.csv\nDue: March 26th 2023"
  },
  {
    "objectID": "assignments/03_hw_causality.html#background",
    "href": "assignments/03_hw_causality.html#background",
    "title": "Problem Set 3: Are Black Legislators Willing to Help African Americans Without Political Incentives?",
    "section": "Background",
    "text": "Background\nThis study conducts a randomized field experiment that examines whether African American politicians are more intrinsically motivated to support African American.\n\nThis exercise is based on: Broockman, D. E. 2013. “Black Politicians Are More Intrinsically Motivated to Advabce Blacks’ Interests: A Frield Experiment Manipulating Political Incentives.” American Journal of Political Science\n\nBroockman conducts a field experiment involving about 7,000 state legislators. Using a putatively black alias, he sent an email asking for help signing up for state unemployment benefits. In the email, the key difference is whether the sender lives within or far from each legislator’s district; the legislator has a more political incentive to respond to the ones within the jurisdiction.\nTo put it simple, Broockman examines whether African American politicians are more likely to respond to the request from an African American even if he is not residing within their jurisdiction.\nThe names and descriptions of variables in the data set aa_rep.csv are:\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nresponse\nResponse from the legislator=1, otherwise=0\n\n\ntreat_out\nResiding outside the district (treatment; 1) or within the district (control; 0)\n\n\nleg_race\nRace of legislator; white=1, black=2, latino=3, native american=4, arab=5, asian=6\n\n\nleg_democrat\nPartisanship; Democrat=1, otherwise=0\n\n\nleg_state\nState of legislator\n\n\nleg_gen\nGender of legislator (female=1; otherwise=0)"
  },
  {
    "objectID": "assignments/03_hw_causality.html#question-1",
    "href": "assignments/03_hw_causality.html#question-1",
    "title": "Problem Set 3: Are Black Legislators Willing to Help African Americans Without Political Incentives?",
    "section": "Question 1",
    "text": "Question 1\nLoad the tidyverse package in the setup chunk. Read the data \"aa_rep.csv\" into R using read_csv. How many observations are in the dataset? After that, create a table that includes average values for all binary variables (such as treat_out, leg_democrat, response, and leg_gen) with knitr::kable() to produce a nice looking table. Describe each variable in the table in a plain language. For example, how many female legislators are included in the sample (the proportion)? What percentages of legislators replied to the email request?"
  },
  {
    "objectID": "assignments/03_hw_causality.html#question-2",
    "href": "assignments/03_hw_causality.html#question-2",
    "title": "Problem Set 3: Are Black Legislators Willing to Help African Americans Without Political Incentives?",
    "section": "Question 2",
    "text": "Question 2\nIn the previous question, we left out leg_race. Let’s look at how many legislators are in the dataset by race in treatment and control groups. Create a nice looking table that shows racial categories in rows and experimental conditions in columns."
  },
  {
    "objectID": "assignments/03_hw_causality.html#question-3",
    "href": "assignments/03_hw_causality.html#question-3",
    "title": "Problem Set 3: Are Black Legislators Willing to Help African Americans Without Political Incentives?",
    "section": "Question 3",
    "text": "Question 3\nThe benefit of randomly assigning individuals to the treatment or control groups is that the two groups would have similar characteristics in terms of pre-treatment variables, on average.\nFirst, let’s create a new binary variable (or vector) for African American legislators, called leg_black indicating black=1 (otherwise=0).\nSecond, using the group_by and summarize functions, create a tibble called balance_table that shows the proportion of African American, Democrat, and women legislators between treatment and control groups in the dataset (calculating the average values). Then use the knitr::kable() function to create a nice-looking table, including some informative column names. Briefly comment on the whether you think these variables appear balanced. If balanced, what does this mean? If not balanced, what should we do?"
  },
  {
    "objectID": "assignments/03_hw_causality.html#question-4",
    "href": "assignments/03_hw_causality.html#question-4",
    "title": "Problem Set 3: Are Black Legislators Willing to Help African Americans Without Political Incentives?",
    "section": "Question 4",
    "text": "Question 4\nBefore answering some of the research questions, now you are wondering legislators in which states are the most responsive. Let’s calculate the average response rates by states first and then create a bar plot with decending order."
  },
  {
    "objectID": "assignments/03_hw_causality.html#question-5",
    "href": "assignments/03_hw_causality.html#question-5",
    "title": "Problem Set 3: Are Black Legislators Willing to Help African Americans Without Political Incentives?",
    "section": "Question 5",
    "text": "Question 5\nOne of the questions that can be answered with the data is whether legislators would be more (or less) responsive if the email request is from the same district where legislators work (other districts that are away from their jurisdictions).\nTo answer this question, we can calculate the SATE (Sample Average Treatment Effects) using the following two variables: response and treat_out.\nDo legislators respond to the unemployment benefit request more if the email sender lives within the district?"
  },
  {
    "objectID": "assignments/03_hw_causality.html#question-6",
    "href": "assignments/03_hw_causality.html#question-6",
    "title": "Problem Set 3: Are Black Legislators Willing to Help African Americans Without Political Incentives?",
    "section": "Question 6",
    "text": "Question 6\nDescribe, in words, what the outcomes are in the analysis for the last problem. Substantively, what does the fundamental problem of causal inference refer to in this context? Make sure to refer to what treatment and control means in this experiment rather than just mention the “treatment” and “control” groups."
  },
  {
    "objectID": "assignments/03_hw_causality.html#question-7",
    "href": "assignments/03_hw_causality.html#question-7",
    "title": "Problem Set 3: Are Black Legislators Willing to Help African Americans Without Political Incentives?",
    "section": "Question 7",
    "text": "Question 7\nThe email sender name signals the legislators his race. In this question, we would like to know whether African American legislators are more likely to respond to the request even if the sender lives in outside their jurisdiction compared to all other legislators [leg_black]."
  },
  {
    "objectID": "assignments/03_hw_causality.html#question-8-optional",
    "href": "assignments/03_hw_causality.html#question-8-optional",
    "title": "Problem Set 3: Are Black Legislators Willing to Help African Americans Without Political Incentives?",
    "section": "Question 8 [optional]",
    "text": "Question 8 [optional]\nNow we are sort of wondering whether other legislators, especially racial minorities, would be willing to help the request from an African American. Let’s calculate the SATE by race of legislators and create a tibble table (the nice looking one). The table should be either 2 (treated and control) x 6 (each race) or 6x2.\nOnce you calculate SATEs by race of legislators, then, create a bar plot using the table. The figure should depict the bars by ascending order. You can choose your own color for the bar plot!"
  },
  {
    "objectID": "assignments/04_hw_summarizing.html",
    "href": "assignments/04_hw_summarizing.html",
    "title": "Problem Set 4: Sources of Empathy in the Circuit Courts",
    "section": "",
    "text": "Data: judges.csv"
  },
  {
    "objectID": "assignments/04_hw_summarizing.html#background",
    "href": "assignments/04_hw_summarizing.html#background",
    "title": "Problem Set 4: Sources of Empathy in the Circuit Courts",
    "section": "Background",
    "text": "Background\nIn this problem set, we will analyze the relationship between the gender composition among a judge’s children and voting behavior among circuit court judges. Adam N. Glynn and Maya Sen argue that having a female child causes circuit court judges to make more pro-feminist decisions. The paper can be found at:\n\nGlynn, Adam N., and Maya Sen. (2015). “Identifying Judicial Empathy: Does Having Daughters Cause Judges to Rule for Women’s Issues?.” American Journal of Political Science Vol. 59, No. 1, pp. 37–54.\n\nThe dataset judges.csv contains the following variables about individual judges:\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nname\nThe judge’s name\n\n\nnum_kids\nThe number of children each judge has.\n\n\ncircuit\nWhich federal circuit the judge serves in.\n\n\ngirls\nThe number of female children the judge has.\n\n\nprogressive_vote\nThe proportion of the judge’s votes on women’s issues which were decided in a pro-feminist direction.\n\n\nrace\nThe judge’s race (1 = white, 2 = African-American, 3 = Hispanic, 4 = Asian-American).\n\n\nreligion\nThe judge’s religion (1 = Unitarian, 2 = Episcopalian, 3 = Baptist, 4 = Catholic, 5 = Jewish, 7 = Presbyterian, 8 = Protestant, 9 = Congregationalist, 10 = Methodist, 11 = Church of Christ, 16 = Baha’i, 17 = Mormon, 21 = Anglican, 24 = Lutheran, 99 = unknown).\n\n\nrepublican\nTakes a value of 1 if the judge was appointed by a Republican president, 0 otherwise. Used as a proxy for the judge’s party.\n\n\nsons\nThe number of male children the judge has.\n\n\nwoman\nTakes a value of 1 if the judge is a woman, 0 otherwise.\n\n\nyearb\nThe year the judge was born."
  },
  {
    "objectID": "assignments/04_hw_summarizing.html#question-1",
    "href": "assignments/04_hw_summarizing.html#question-1",
    "title": "Problem Set 4: Sources of Empathy in the Circuit Courts",
    "section": "Question 1",
    "text": "Question 1\nLoad the tidyverse package in the setup chunk. In the first chunk for this question use read_csv to load the judges.csv file into a data frame called judges. In this exercise, you will create a cross-tab that shows the gender breakdown within each party. In particular, the result should be a table with 2 rows and 3 columns, where the rows represent each gender and the columns correspond to party. In the end, you should have a table with correct numbers that looks like this with the correct proportion:\n\n\n\nGender\nDemocrat\nRepublican\n\n\n\n\nMan\n0.??\n0.??\n\n\nWoman\n0.??\n0.??\n\n\n\nNote that the columns here sum to 1. You can create this using the following data wrangling steps:\n\nOverwrite the judges tibble with the same tibble with two newly created variables: one variable called Gender variable to be labelled \"Woman\" and \"Man\" based on the variable woman and another variable called Party to be labelled \"Republican\" and \"Democrat\" based on the republican variable.\nUse the judges tibble to create the cross-tab and save it as gender_party_table. It should have 2 rows and 3 columns (“Gender”, “Republican”, and “Democrat”). Be very careful about the ordering of your grouping and remember that summarize() drops the last group by default when there is more than one group. You should use group_by(), summarize(), mutate(), and pivot_wider() to accomplish this. (Hint: if you are ending up with the wrong number of rows, make sure that the only variables in the tibble when you try to use pivot_wider are the two grouping variables and the proportion variable.)\nPass gender_party_table to knitr::kable() to create a nicely formatted version of this table.\n\nIn your write-up, answer the following questions:\n\nHow many judges are in this data set?\nWhat proportion of the judges are men? (hint: use mean)\nFrom your table, is the gender breakdown different for judges appointed by Democratic vs Republican presidents?"
  },
  {
    "objectID": "assignments/04_hw_summarizing.html#question-2",
    "href": "assignments/04_hw_summarizing.html#question-2",
    "title": "Problem Set 4: Sources of Empathy in the Circuit Courts",
    "section": "Question 2",
    "text": "Question 2\nUse group_by and summarize to calculate the mean of progressive_vote in each combination of the Gender and Party variables, and then save this tibble as gender_party_means (this tibble should have columns Gender, Party, and progressive_vote). Use this table to recreate the following barplot:\n\nYou can pick the barplot colors. Just in case you want to match the figure colors above, they are \"steeblue1\" and \"indianred1\".\nIn the main text, briefly interpret the results of the analysis by describing which party and which gender tend to be more progress. Comment on whether there are larger differences across party or across gender in terms of progressive voting. Should we interpret any of these effects causally? Why or why not?"
  },
  {
    "objectID": "assignments/04_hw_summarizing.html#question-3",
    "href": "assignments/04_hw_summarizing.html#question-3",
    "title": "Problem Set 4: Sources of Empathy in the Circuit Courts",
    "section": "Question 3",
    "text": "Question 3\nWhat is the difference in the proportion of pro-feminist decisions between judges who have at least one daughter and those who do not have any? To compute this difference, first create a variable called any_girls that is \"Any Girls\" when the judge has at least 1 girl and \"No Girls\" otherwise, and save this variable back to the judges tibble. Then, created a new tibble called parents that is filtered to contain judges that have at least one child. Create an object called vote_by_girls that has three columns: the unique values of the any_girls variable, the mean of the progressive_vote variable for each value of any_girls (column called progressive_mean), and the standard deviation of progressive_vote for each value of any_girls (column called progressive_sd). Present the table using knitr::kable().\nIn the main text, describe which group has a higher average progressive vote and which group’s distribution has more spread. Why might we worry about interpreting any difference in means causally, considering number of children as a possible confounder?"
  },
  {
    "objectID": "assignments/04_hw_summarizing.html#question-4",
    "href": "assignments/04_hw_summarizing.html#question-4",
    "title": "Problem Set 4: Sources of Empathy in the Circuit Courts",
    "section": "Question 4",
    "text": "Question 4\nGiven that the number of children might be a confounder for the relationship between number of girls and voting, let’s estimate the average treatment effects of having girls using statistical control for the number of children among judges that have one to three children (that is, first filter to judges that only have between 1 and 3 children, inclusive). Your final table should be called ate_nkids and should be a tibble that has two columns: one with the number of children (1, 2, and 3) and the other with the estimated ATEs of any_girls for each of those levels. Print out this table using the knitr::kable() command. Are these estimated effects largely similar or largely different than what you found using all of the data? What assumption do you need to make to interpret these effects causally? Do you think it is plausible in this case?"
  },
  {
    "objectID": "assignments/04_hw_summarizing.html#question-5-optional",
    "href": "assignments/04_hw_summarizing.html#question-5-optional",
    "title": "Problem Set 4: Sources of Empathy in the Circuit Courts",
    "section": "Question 5 (Optional)",
    "text": "Question 5 (Optional)\nLet’s consider the design of this study. The original authors assume that, conditional on the number of children a judge has, the number of daughters is random (as we did in the previous question). If this is true, half of a judge’s children should be female, on average. A deviation from this proportion could indicate that a gender preference among judges due a stopping rule such as “have children until we get one girl,” which would violate the randomization assumption.\nTo check this assumption, group the data by the number of children and calculate the proportion of children in each group that are girls. Create a barplot that these proportions on the y-axis with the number of children on the x-axis. This barplot should have (a) informative labels on each axis, (b) a y-axis range that runs from 0 to 1, and (c) a horizontal line at 0.5 to compare against. Does it appear that there is strong gender preference/selection happening among judges?"
  },
  {
    "objectID": "assignments/05_hw_regression.html",
    "href": "assignments/05_hw_regression.html",
    "title": "Problem Set 5: Co-ethnic Candidates and Voter Turnout",
    "section": "",
    "text": "Data: blackturnout.csv"
  },
  {
    "objectID": "assignments/05_hw_regression.html#background",
    "href": "assignments/05_hw_regression.html#background",
    "title": "Problem Set 5: Co-ethnic Candidates and Voter Turnout",
    "section": "Background",
    "text": "Background\nFor this problem set, we will analyze data from the following article:\nFraga, Bernard. (2015) “Candidates or Districts? Reevaluating the Role of Race in Voter Turnout,” American Journal of Political Science, Vol. 60, No. 1, pp. 97–122.\nFraga assesses the theory that minority voters are more likely to vote in elections featuring co-ethnic candidates. He shows that the association between minority voter turnout and co-ethnic candidates disappears once we take into account district-level racial composition. In particular, he demonstrates that in districts where Black people make up a greater share of the voting-age population, Black voters in that district are more likely to vote in elections regardless of candidate race. Although the main analyses he carries out are a bit more complicated than what have learned in the course, we can replicate his substantive findings using the multiple regression approach we’ve learned.\nA description of the variables is listed below:\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nyear\nYear the election was held\n\n\nstate\nState in which the election was held\n\n\ndistrict\nDistrict in which the election was held (unique within state but not across states)\n\n\nturnout\nThe proportion of the Black voting-age population in a district that votes in the general election\n\n\nCVAP\nThe proportion of a district’s voting-age population that is Black\n\n\ncandidate\nBinary variable coded “1” when the election includes a Black candidate; “0” when the election does not include a Black candidate"
  },
  {
    "objectID": "assignments/05_hw_regression.html#question-1",
    "href": "assignments/05_hw_regression.html#question-1",
    "title": "Problem Set 5: Co-ethnic Candidates and Voter Turnout",
    "section": "Question 1",
    "text": "Question 1\nFraga analyzes turnout data for four different racial and ethnic groups, but for this analysis we will focus on the data for Black voters. Load data/blackturnout.csv and save it as blackturnout.\nIn the write up, indicate which years are included in the dataset and how many different states are included in the dataset."
  },
  {
    "objectID": "assignments/05_hw_regression.html#question-2",
    "href": "assignments/05_hw_regression.html#question-2",
    "title": "Problem Set 5: Co-ethnic Candidates and Voter Turnout",
    "section": "Question 2",
    "text": "Question 2\nCreate a boxplot that compares Black turnout in elections with and without a co-ethnic candidate and save the plot as turnout_box. Your plot should look like this:\n\nTo create the x-axis variable that is nicely labeled, use mutate() to create a variable that is \"Yes\" when candidate is 1 and \"No\" otherwise (while doing so, be careful not to overwrite your candidate variable!). Be sure to use informative labels, though they do not have to match the text exactly.\nIn the write-up, report what the plot tells us about how turnout varies by the presence of a co-ethnic candidate."
  },
  {
    "objectID": "assignments/05_hw_regression.html#question-3",
    "href": "assignments/05_hw_regression.html#question-3",
    "title": "Problem Set 5: Co-ethnic Candidates and Voter Turnout",
    "section": "Question 3",
    "text": "Question 3\nRun a linear regression with Black voter turnout as your outcome variable and candidate co-ethnicity as your predictor. Save this regression as fit_1 and report the coefficients using a nicely formatted table with the following code (you may need to install the broom package to have this work):\n\nfit_1 |>\n  broom::tidy() |>\n  select(term, estimate) |>  \n  knitr::kable(digits = 2)\n\nInterpret both of these coefficients. Do not merely comment on the direction of the association (i.e., whether the slope is positive or negative). Explain what the value of the coefficients mean in terms of the units in which each variable is measured. Are these results consistent with the prediction that Black voters turn out at higher rates when a co-ethnic candidate is running?"
  },
  {
    "objectID": "assignments/05_hw_regression.html#question-4",
    "href": "assignments/05_hw_regression.html#question-4",
    "title": "Problem Set 5: Co-ethnic Candidates and Voter Turnout",
    "section": "Question 4",
    "text": "Question 4\nYou decide to investigate the results of the previous question a bit more carefully because the elections with co-ethnic candidates may differ from the elections without co-ethnic candidates in other ways. Create a scatter plot where the x-axis is the proportion of voting-age population that is Black and the y-axis is Black voter turnout. Color your points according to candidate co-ethnicity. That is, make the points for elections featuring co-ethnic candidates one color, and make the points for elections featuring no co-ethnic candidates a different color. You plot should look like this:\n\nYou should save this plot as turnout_scatter and to create better labels for the color of the points, use mutate() in a similar way to question 2.\nAnswer these questions in the write-up: What does this graph seem to imply about the relationship between Black voting-age population and Black turnout? What does it tell us about the relationship between Black voting-age population and the presence of a Black candidate?"
  },
  {
    "objectID": "assignments/05_hw_regression.html#question-5",
    "href": "assignments/05_hw_regression.html#question-5",
    "title": "Problem Set 5: Co-ethnic Candidates and Voter Turnout",
    "section": "Question 5",
    "text": "Question 5\nRun a linear regression with Black turnout as your outcome variable and with candidate co-ethnicity and co-ethnic voting-age population as your predictors. Save the output of this regression as fit_2 and report the coefficients using a nicely formatted table with the following code (you may need to install the broom package to have this work):\n\nfit_2 |>\n  broom::tidy() |>\n  select(term, estimate) |>  \n  knitr::kable(digits = 2)\n\nIn the main text, interpret the coefficients on the two predictors, ignoring the intercept for now (you will interpret the intercept in the next question). Explain what each coefficient represents in terms of the units of the relevant variables."
  },
  {
    "objectID": "assignments/05_hw_regression.html#question-6",
    "href": "assignments/05_hw_regression.html#question-6",
    "title": "Problem Set 5: Co-ethnic Candidates and Voter Turnout",
    "section": "Question 6",
    "text": "Question 6\nNow interpret the intercept from the regression model with two predictors. Is this intercept a substantively important or interesting quantity? Why or why not?"
  },
  {
    "objectID": "assignments/05_hw_regression.html#question-7",
    "href": "assignments/05_hw_regression.html#question-7",
    "title": "Problem Set 5: Co-ethnic Candidates and Voter Turnout",
    "section": "Question 7",
    "text": "Question 7\nComparing the two regression models you have fit, what do you conclude about the relationship between co-ethnic candidates and Black voter turnout? Do they come to similar or different conclusions about this relationship? Which model do you prefer and why? (Please ignore issues of statistical significance for this question given that it will be covered later in the course.)"
  },
  {
    "objectID": "assignments/06_hw_sampling.html",
    "href": "assignments/06_hw_sampling.html",
    "title": "Problem Set 6: Sampling from a Voter File",
    "section": "",
    "text": "In this exercise, we will focus on sampling and sampling distributions when we have access to an entire census for a given population. In this case, the data/fulton.csv file contains anonymized data on all registered voters in Fulton County, GA from 1994. The variables in this dataset are:\n\n\n\nName\nDescription\n\n\n\n\nturnout\ndid person vote (1) or not (0) in 1994?\n\n\nblack\nis this person black (1) or not (0)?\n\n\nsex\nis this person a woman (1) or not (0)?\n\n\nage\nage of registered voter\n\n\ndem\nregistered as a Democrat (1) or not (0)?\n\n\nrep\nregistered as a Republican (1) or not (0)?\n\n\nurban\nregistered in a city (1) or not (0)?\n\n\n\nFor the purposed of this exercise, we will treat this data as the population of interest. Doing so is an increasingly common approach for polling, where pollsters are now using the voter file as a sampling frame to conduct their polls. We will repeated sample from this population to better understand sampling uncertainty.\nNote: please follow the directions carefully about setting the seed for the sampling based questions."
  },
  {
    "objectID": "assignments/06_hw_sampling.html#question-1",
    "href": "assignments/06_hw_sampling.html#question-1",
    "title": "Problem Set 6: Sampling from a Voter File",
    "section": "Question 1",
    "text": "Question 1\nLoad the voter list data into R using read_csv and save the data as fulton.\nCreate a density histogram of age with a bin width of 1 and save this plot as age_hist (use the aesthetic mapping y = ..density.. in to accomplish this). Create a barplot for turnout with the proportion on the y-axis (use the aesthetic mapping y = ..prop.. in geom_barplot to achieve this). Make sure both of these plots are shown in the PDF.\nIn the write-up, state how many units are in the population (that is, how many rows are in the fulton data)."
  },
  {
    "objectID": "assignments/06_hw_sampling.html#question-2",
    "href": "assignments/06_hw_sampling.html#question-2",
    "title": "Problem Set 6: Sampling from a Voter File",
    "section": "Question 2",
    "text": "Question 2\nUse summarize() to calculate the population mean and standard deviation of age and turnout (that is the mean and SD of these variables in the fulton data) and save the resulting tibble as pop_parameters with the tibble output looking like:\n# A tibble: 1 × 4\n  age_pop_mean age_pop_sd turnout_pop_prop turnout_pop_sd\n         <dbl>      <dbl>            <dbl>          <dbl>\n1         XX.X       XX.X            X.XXX          X.XXX\nMake sure that the column names are the same (Hint: you can summarize multiple variables in the same call to summarize). Use knitr::kable() to present the values in nicely formatted table with digits = 2 to create nicely rounded numbers and informative column names (they may need to be abbreviated to fit on the page)."
  },
  {
    "objectID": "assignments/06_hw_sampling.html#question-3",
    "href": "assignments/06_hw_sampling.html#question-3",
    "title": "Problem Set 6: Sampling from a Voter File",
    "section": "Question 3",
    "text": "Question 3\nIn the first line of the code chunk for this question use the following code:\n\nlibrary(infer)\nset.seed(02138)\n\nThen use rep_slice_sample() to take 1,000 samples of size 20 from fulton and calculate the sample mean of age and the sample mean/proportion of turnout for each of these samples. Save these as variables named age_mean and turnout_prop and save the resulting tibble as samples_n20.\nCreate a density histogram of the age means and with a bin width of 1 and save this as age_mean_hist. Create a density histogram of the turnout proportions and save this as turnout_prop_hist. Make sure both of these plots are shown in the PDF and that they have informative labels.\nIn the write-up, compare the sampling distribution of the sample mean and sample proportion here to the population distributions from question 1. Are the shapes of the sampling distributions similar or different to the population distributions? If different, how are they different?"
  },
  {
    "objectID": "assignments/06_hw_sampling.html#question-4",
    "href": "assignments/06_hw_sampling.html#question-4",
    "title": "Problem Set 6: Sampling from a Voter File",
    "section": "Question 4",
    "text": "Question 4\nUse the summarize() function on samples_n20 to calculate the average (named ev_age and ev_turnout) and standard deviation (named se_age and se_turnout) of each sample mean/proportion across the repeated samples. Save this tibble as samp_dist_summary and it should look like this:\n# A tibble: 1 × 4\n  ev_age se_age ev_turnout se_turnout\n   <dbl>  <dbl>      <dbl>      <dbl>\n1   X.XX   X.XX      X.XXX      X.XXX\nMake sure that the column names are the same. Use knitr::kable() to present the values in nicely formatted table with digits = 2 to create nicely rounded numbers.\nCompare the mean and SD of these sampling distributions to the population means and SDs from the previous question. Are these distributions centered on the same value? Which has more spread, the population distribution of age/turnout or the sampling distributions of their means?"
  },
  {
    "objectID": "assignments/06_hw_sampling.html#question-5",
    "href": "assignments/06_hw_sampling.html#question-5",
    "title": "Problem Set 6: Sampling from a Voter File",
    "section": "Question 5",
    "text": "Question 5\nThe central limit theorem says that sums and means tend to be normally distributed so that 68% of the sampling distribution of the mean should be within 1 standard deviation of the expected value of the expected value. Let’s see if this if this approximation is good for our sampling distributions.\nUse mutate to create a new variable in samples_n20 called age_error that is the absolute value of the difference between age_mean and the average of the age_mean. Then create a variable called within_1sd_age that is TRUE is this absolute difference is less than or equal to the standard deviation of age_mean. Finally, take the mean of this variable to obtain the proportion of sample means that are within one SD of the mean of their distribution. Save the resulting 1x1 tibble as age_clt_n20.\nTo get you started, you can calculate the absolute value of the difference between a variable x and its mean using the following code:\n\nmydata |>\n  mutate(\n    error = abs(x - mean(x))\n  )\n\nReport this proportion in the main text and comment on whether it is similar to what the CLT would predict."
  },
  {
    "objectID": "assignments/06_hw_sampling.html#question-6",
    "href": "assignments/06_hw_sampling.html#question-6",
    "title": "Problem Set 6: Sampling from a Voter File",
    "section": "Question 6",
    "text": "Question 6\nIn this question you will repeat the exercise from question 4, using turnout_prop instead.\nUse mutate to create a new variable in samples_n20 called turnout_error that is the absolute value of the difference between turnout_prop and the average of the turnout_prop. Then create a variable called within_1sd_turnout that is TRUE is this absolute difference is less than or equal to the standard deviation of turnout_prop. Finally, take the mean of this variable to obtain the proportion of sample means that are within one SD of the mean of their distribution. Save the resulting 1x1 tibble as turnout_clt_n20.\nReport this proportion in the main text and comment on whether it is similar to what the CLT would predict. If this is different than age, can you think of anything about the two variables that differ that might cause the CLT approximation to be better for one than the other?"
  },
  {
    "objectID": "assignments/06_hw_sampling.html#question-7",
    "href": "assignments/06_hw_sampling.html#question-7",
    "title": "Problem Set 6: Sampling from a Voter File",
    "section": "Question 7",
    "text": "Question 7\nThis problem is optional.\nWrite the following line at the beginning of the code chunk for this problem:\n\nset.seed(02138)\n\nThen create a tibble called samples_n200 that replicates the exercise the sampling of question 3 but with 1,000 samples of size 200. With this tibble replicate the analysis of questions 4 and 5 to get the proportion of sample means/proportions age_mean and turnout_prop that are within 1 SD of the means of those distributions. You can save the resulting 1x1 tibbles for age as age_clt_n200 and for turnout as turnout_clt_n200.\nIn the write-up, report these values and answer the following questions. Does the normal approximation seem better here than with a sample size of 20? Which variable sees more improvement in the approximation?"
  },
  {
    "objectID": "assignments/07_hw_bootstrap.html",
    "href": "assignments/07_hw_bootstrap.html",
    "title": "Problem Set 7: Exposure to Inequality and Support for Redistribution",
    "section": "",
    "text": "Does exposure to inequality affect our support for redistributive policies such as taxes on higher income earners? A recent paper explored the effect of brief exposure to socioeconomic inequality in an everyday setting on support for a millionaire’s tax. This exercise is based on:\n\nSands, Melissa L. 2017. “Exposure to inequality affects redistribution.” Proceedings of the National Academy of Sciences, 114(4): 663-668.\n\nIn this experiment, the author hired actors to stand in affluent, predominantly white, commercial areas around Boston, MA that have high pedestrian traffic. These actors were either white or Black, and each actor dressed in attire that indicated either affluence (well-dressed, well-groomed) or poverty (unkempt, shabby clothing). The author randomly assigned shifts to each actor with randomly chosen attire to stand on a city street within 20 feet of a petitioner hired by the researcher. This petitioner would stop every third adult that walked past the actor and ask them to sign a petition for the millionaire’s tax (a measure in MA to impose an additional tax of 4% on individuals with annual incomes of $1 million or more) or to sign a petition about reducing the use of plastic bags in local stores. The type of petition was randomly assigned as well. The outcome of interest is whether the respondent agreed to sign the petition on the millionaire’s tax (the plastic bag petition is used as a placebo).\nA total of 2,591 respondents were petitioned with 1,335 being petitioned about the millionaire’s tax. Petitioners also collected their “best guess” about the gender, age, and race/ethnicity of each person approached. The data file for this study is inequality-exposure.csv and contains the following variables:\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nsigned\n1 if the respondent signed the petition, 0 otherwise\n\n\nmill_tax\n1 if petitioned about the millionaire’s tax, 0 for plastic bag petition.\n\n\nblackactor\n1 if actor was Black for this respondent, 0 for white\n\n\npooractor\n1 if actor was in poverty condition, 0 for affluent condition\n\n\nblack\n1 if petitioner guessed respondent was Black\n\n\nwhite\n1 if petitioner guessed respondent was non-Hispanic white\n\n\nasian\n1 if petitioner guessed respondent was Asian\n\n\nhisp\n1 if petitioner guessed respondent was Hispanic\n\n\nyoung\n1 if petitioner guessed respondent was 18-35 years old\n\n\nmiddle\n1 if petitioner guessed respondent was 36-65 years old\n\n\nold\n1 if petitioner guessed respondent was >65 years old\n\n\nfemale\n1 if petitioner guessed respondent was female\n\n\nclust\nCluster number of respondent (see question 6)"
  },
  {
    "objectID": "assignments/07_hw_bootstrap.html#question-1",
    "href": "assignments/07_hw_bootstrap.html#question-1",
    "title": "Problem Set 7: Exposure to Inequality and Support for Redistribution",
    "section": "Question 1",
    "text": "Question 1\nLoad the data into R and name it ineq. Create a tibble called mill_df that is filtered to respondents petitioned about the millionaire’s tax. We will use this data throughout the exercise. Create two new variables:\n\ncostume that is \"Poor\" when pooractor is 1 and \"Affluent\" otherwise\nrace_actor that is \"Black\" when blackactor is 1 and \"White\" otherwise\n\nCalculate the following object, saving it with the names indicated:\n\nineq_diff: The difference in means in petition signing (signed) between seeing the actor in the poor and affluent conditions (costume) for those who were petitioned about the millionaire’s tax. This should be a 1x1 tibble.\n\nReport this values in the text of your write up and briefly interpret it."
  },
  {
    "objectID": "assignments/07_hw_bootstrap.html#question-2",
    "href": "assignments/07_hw_bootstrap.html#question-2",
    "title": "Problem Set 7: Exposure to Inequality and Support for Redistribution",
    "section": "Question 2",
    "text": "Question 2\nIn the first line of the code chunk for this question use the following code:\n\nlibrary(infer)\nset.seed(02138)\n\nGenerate 1,000 bootstrap replications of the estimated ATE from Question 1 and save these bootstraps in a tibble called ate_boots. You may use the rep_slice_sample() or specify/generate approach, but the column of bootstrapped ATEs should either be called ATE or stat.\nUse these bootstraps to calculate a 95% confidence interval for the difference in means using the percentile method and save this as ate_ci_95, which should be a 1 by 2 tibble.\nUse ggplot(), geom_histogram() to plot the bootstrap distribution and overlay it with the confidence interval using this geom:\n\ngeom_vline(xintercept = unlist(ate_ci_95))\n\nThis will be manually graded in the PDF so be sure it shows up in the PDF. Use informative labels.\nIn the writeup, discuss if the CI contains zero? What does that mean?"
  },
  {
    "objectID": "assignments/07_hw_bootstrap.html#question-3",
    "href": "assignments/07_hw_bootstrap.html#question-3",
    "title": "Problem Set 7: Exposure to Inequality and Support for Redistribution",
    "section": "Question 3",
    "text": "Question 3\nExplain how to interpret 95% confidence intervals in terms of repeated sampling. Is it possible to produce a 100% confidence interval in this setting? If so, what is it and is it useful?"
  },
  {
    "objectID": "assignments/07_hw_bootstrap.html#question-4",
    "href": "assignments/07_hw_bootstrap.html#question-4",
    "title": "Problem Set 7: Exposure to Inequality and Support for Redistribution",
    "section": "Question 4",
    "text": "Question 4\nCalculate the ATE for Black and White actors separately (using the race_actor variable) and calculate the interaction or difference between these two ATEs. The output should be a 1 row tibble named ate_race with columns ATE_Black, ATE_White, and ATE_Diff that are the ATE for Black actors, White actors and the difference between them, respectively.\nIn the write-up, report the interaction and describe what it means in the substance of this experiment."
  },
  {
    "objectID": "assignments/07_hw_bootstrap.html#question-5",
    "href": "assignments/07_hw_bootstrap.html#question-5",
    "title": "Problem Set 7: Exposure to Inequality and Support for Redistribution",
    "section": "Question 5",
    "text": "Question 5\nIn the first line of the code chunk for this question use the following code:\n\nset.seed(02138)\n\nUse rep_slice_sample (not specify/generate) to generate 1000 boostrap replications of the difference in ATEs between Black and White actors from Question 4. Save this tibble as ate_race_boots.\nThen construct a 95% confidence interval for the difference between the ATE for black Actors and the ATE for White actors and save this confidence intervals as ate_race_ci_95.\nUse ggplot(), geom_histogram() to plot the bootstrap distribution and overlay it with the confidence interval using this geom:\n\ngeom_vline(xintercept = unlist(ate_race_ci_95))\n\nThis will be manually graded in the PDF so be sure it shows up in the PDF. Use informative labels.\nIn the writeup, discuss if the CI contains zero? What does that mean?"
  },
  {
    "objectID": "assignments/08_hw_hyp_tests.html",
    "href": "assignments/08_hw_hyp_tests.html",
    "title": "Problem Set 8: The Effect of Cognitive Behavioral Therapy on Crime and Violence",
    "section": "",
    "text": "Can noncognitive skills and preferences like patience and identity be changed in adults? And can improving these skills lower crime and violence? A paper attempted to answer these questions with a randomized field experiment in Liberia. This exercise is based on:\n\nBlattman, Christopher, Julian C. Jamison, and Margaret Sheridan. 2017. “Reducing Crime and Violence: Experimental Evidence from Cognitive Behavioral Therapy in Liberia.” American Economic Review, 107(4): 1165-1206.\n\nIn this experiment, the researchers randomly assigned two different treatments to all respondents. Half of respondents were randomly assigned to eight weeks of cognitive behavioral therapy designed to reduce self-destructive beliefs or behaviors and promote positive ones. The sessions focused on encouraging goal-setting and self-control with simple behaviors and then built up to dealing with more realistic situations while learning to control emotions. The was to promote noncognitive skills that might reduce tendencies toward criminal activity or violence. Note that not everyone assigned to treatment actually attended all of the meetings. The second treatment was a randomly assigned cash prize of $200. The outcomes of interest are index measures of anti-social behavior like theft, carrying weapons, or selling drugs. An index combines all of these different variables into one continuous measure. The researchers also have baseline measures of some of these variables.\nA total of 999 respondents were randomized in the study. The data file for this study is data/cbt.csv and contains the following variables:\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\npartid\nparticipant ID number\n\n\ncashassigned\nparticipant assigned to cash treatment (1) or not (0)\n\n\ntpassigned\nparticipant assigned to CBT treatment (1) or not (0)\n\n\nattend_80\ndid the participant attend 80% CBT meetings (1) or not (0)\n\n\nsteals_basline\nhas the participant committed theft in the last 2 weeks before treatment\n\n\nhomeless_basline\nis the participant sleeping on the streets before treatment\n\n\nyear_born\nyear of birth of participant\n\n\nfam_asb_st\nindex score of anti-social behaviors 2 weeks posttreatment\n\n\ncarryweapon\ndoes participant carry a weapon, 2 weeks posttreatment\n\n\nfam_asb_lt\nindex score of anti-social behaviors 12 months posttreatment"
  },
  {
    "objectID": "assignments/08_hw_hyp_tests.html#question-1",
    "href": "assignments/08_hw_hyp_tests.html#question-1",
    "title": "Problem Set 8: The Effect of Cognitive Behavioral Therapy on Crime and Violence",
    "section": "Question 1",
    "text": "Question 1\nExperiments are designed so that treatment assignment is unrelated to any background characteristics of the participants. But treatment and control groups will differ a bit by random chance even if they are the same on average. Let’s see if the randomization created groups that are fairly balanced by doing a balance test.\nLoad the tidyverse and infer packages and load the data into R and name it cbt. Create two new variables in the cbt tibble:\n\ncbt_assigned that is \"CBT\" if tpassigned is 1 and \"No CBT\" otherwise.\nunhoused_baseline that is \"Unhoused\" if homeless_baseline is 1 and \"Housed\" otherwise.\n\nCalculate the difference in proportions in the baseline measure of sleeping on the streets (unhoused_baseline) between the CBT treatment group and the non-CBT group (cbt_assigned). Save this value as base_diff. Next, use the infer framework to conduct a two-sided permutation hypothesis test of whether the CBT treatment group has the same proportion of unhoused_baseline as the non-CBT group. Calculate the two-sided p-value using get_p_value() and save it as base_p.\nReport the observed difference and the p-value in the write-up. With an \\(\\alpha\\) of 0.05, can you reject the null hypothesis that the proportion sleeping on the streets is the same in the two groups? Given that decision, which type of error (type I vs type II) would be worried for this test?\nNOTE: For this problem, set the seed to 02138 at the top of the chunk and use reps = 1000 when generating the permutations."
  },
  {
    "objectID": "assignments/08_hw_hyp_tests.html#question-2",
    "href": "assignments/08_hw_hyp_tests.html#question-2",
    "title": "Problem Set 8: The Effect of Cognitive Behavioral Therapy on Crime and Violence",
    "section": "Question 2",
    "text": "Question 2\nA colleague suggests that you should use actual attendance at the CBT meetings rather than the random assignment since there shouldn’t be any treatment effect for those that don’t attend the meetings. To investigate this, you decide to do a balance test on the attendance variable that measures if a person actually attended 80% of the CBT meetings (attend_80). Create a new variable:\n\ncbt_attended that is \"Attended CBT\" if attend_80 is 1 and \"Not Attended\" otherwise.\n\nCalculate the difference in proportions in baseline sleeping on the streets (unhoused_baseline) for those that actually attended 80% of meetings versus those that did not (cbt_attended) and save this difference as base_diff_attend. Calculate the two-sided p-value for another permutation hypothesis for the null hypothesis that the true proportion of baseline sleeping on the street is the same across values of attend_80. Save this p-value as base_p_attend.\nReport the observed difference and the p-value in the write-up. Is any imbalance that you find on this variable statistically significant (that is, can you reject the null hypothesis) if \\(\\alpha\\) is 0.05? Which type of error are we worried about here? What parameter of the hypothesis test can control this type of error?\nNOTE: For this problem, set the seed to 02138 at the top of the chunk and use reps = 1000 when generating the permutations."
  },
  {
    "objectID": "assignments/08_hw_hyp_tests.html#question-3",
    "href": "assignments/08_hw_hyp_tests.html#question-3",
    "title": "Problem Set 8: The Effect of Cognitive Behavioral Therapy on Crime and Violence",
    "section": "Question 3",
    "text": "Question 3\nExplain how the tests in questions 1 and 2 should inform whether it is better to use the CBT assignment or the actually attendence to the CBT meetings as the treatment variable when estimating causal effects."
  },
  {
    "objectID": "assignments/08_hw_hyp_tests.html#question-4",
    "href": "assignments/08_hw_hyp_tests.html#question-4",
    "title": "Problem Set 8: The Effect of Cognitive Behavioral Therapy on Crime and Violence",
    "section": "Question 4",
    "text": "Question 4\nCalculate the estimated ATE of the CBT treatment assignment on the short-term index measure of anti-social behavior (fam_asb_st) and save the estimate as ate.\nGenerate 1000 simulations from the null distribution of a permutation hypothesis test of the null hypothesis that there is no treatment effect. Save this distribution as ate_null_dist. Plot this distribution of the difference in means under the null hypothesis along with the observed estimate. Explain, in substantive terms of this setting, what this distribution represents.\nCalculate the two-sided p-value and save it as ate_p. Is the estimated effect statistically significant when alpha is 0.05?\nNOTE: For this problem, set the seed to 02138 at the top of the chunk and use reps = 1000 when generating the permutations."
  },
  {
    "objectID": "assignments/08_hw_hyp_tests.html#question-5",
    "href": "assignments/08_hw_hyp_tests.html#question-5",
    "title": "Problem Set 8: The Effect of Cognitive Behavioral Therapy on Crime and Violence",
    "section": "Question 5",
    "text": "Question 5\nCalculate the estimated ATE of the CBT treatment assignment on the long-term index measure of anti-social behavior (fam_asb_st) and save the estimate as ate_lt.\nGenerate 1000 simulations from the null distribution of a permutation hypothesis test of the null hypothesis that there is no treatment effect. Save this distribution as ate_lt_null_dist. Plot this distribution of the difference in means under the null hypothesis along with the observed estimate and the shaded region for the p-value using visualize() and shade_p_value().\nCalculate the two-sided p-value and save it as ate_lt_p. Is the estimated effect statistically significant when alpha is 0.05?\nIn the write up, describe what this and the ATE estimated in question 4 mean for the persistence of the effect of CBT over time?\nNOTE: For this problem, set the seed to 02138 at the top of the chunk and use reps = 1000 when generating the permutations."
  },
  {
    "objectID": "assignments/final-project.html",
    "href": "assignments/final-project.html",
    "title": "Final Project Information (optional)",
    "section": "",
    "text": "The final project is a data analysis project about whatever data excites you. No matter the topic, you will formulate a key research question, find data on that question, answer the question using the tools of the course, and present those results for public consumption.\nHere is a list of milestones that we will have to keep you on track:"
  },
  {
    "objectID": "assignments/final-project.html#milestone-1-finding-data-and-writing-a-proposal-due-49",
    "href": "assignments/final-project.html#milestone-1-finding-data-and-writing-a-proposal-due-49",
    "title": "Final Project Information (optional)",
    "section": "Milestone 1: Finding data and writing a proposal (due 4/9)",
    "text": "Milestone 1: Finding data and writing a proposal (due 4/9)\n\nFinding a data source\nThe biggest part of the final project is finding a data source. If you want to utilize data from TPD, feel free to do so; I would be happy to help tidying up the data and analyzing those! You may also find more cleaned data from some other resources. Here are examples:\n\nList of links to political science data sets\nHarvard Dataverse - Social Science\nData.gov - Data sets released by the US government\nData published by FiveThirtyEight\nPew Research Center Data Sets\n\nIf you find a data set that you think is interesting, but you have problems loading the data set into R, I am happy to help you loading the data. R can certainly load almost any data.\n\n\nGeneral advice for choosing data sources\n\nIf you want to analyze the relationship between X and Y, make sure that these two variables are included in the data set. If you want to look at effects for subgroups, make sure there is a variable that you can use for subsetting.\nTry to look for a ‘codebook’ or some other document that explains what the variables mean and how they are coded.\nFor most projects, preparing the data for analysis takes longer than the actual analysis itself. Try to find a data set where you do not need to extensively recode / clean up the data before you run your analyses, this makes the final project easier.\nIn similar vein, if the data set is greater than about 50MB (this is not a hard cutoff), R commands and analyses tend to take longer.\nData from experiments is usually simple to analyze, since the analysis commonly involves simple comparisons of group means.\n\n\n\nWriting a proposal\nYou should write a one-paragraph note to describe what data set you will use and what your tentative research question is. Your research question should ask how one dependent variable is related to one or more independent variables. That is, your research question should be able to be answered by a regression analysis. In this paragraph, you should do the following:\n\nState your research question.\nFormulate a hypothesis related to the research question. This hypothesis should be rooted in some sort of theory. In other words, you need to present a plausible story why the hypothesis might be true. Often, this is in the form of a behaviorial or institutional explanation. As social scientists, we are not interested in idiosyncratic explanations; we want to understand systematic patterns and relationships!\nDescribe your explanatory variable(s) of interest and how it is measured. Importantly, we need to observe variation in this variable in order to study it!\nDescribe your outcome variable of interest and how it is measured.\nWhat observed pattern in the data would provide support for your hypothesis? More importantly, what observed pattern would disprove your hypothesis?\n\nNote that you are not fully committing to any specific question or data in this exercise. If you want to change data or questions later, that is fine. This is just a milestone to keep you on track."
  },
  {
    "objectID": "assignments/final-project.html#milestone-2-data-visualization-and-analysesdue-416",
    "href": "assignments/final-project.html#milestone-2-data-visualization-and-analysesdue-416",
    "title": "Final Project Information (optional)",
    "section": "Milestone 2: Data visualization and analyses(due 4/16)",
    "text": "Milestone 2: Data visualization and analyses(due 4/16)\nLoads the data you have selected and produces one interesting and polished data visualization. This could either show the distribution of one variable or the relationship between two variables. Also produce one analysis that attempts to answer your research question."
  },
  {
    "objectID": "assignments/final-project.html#final-step-write-up-final-report-due-57",
    "href": "assignments/final-project.html#final-step-write-up-final-report-due-57",
    "title": "Final Project Information (optional)",
    "section": "Final step: Write up final report (due 5/7)",
    "text": "Final step: Write up final report (due 5/7)\nThe final report will include the following sections: (1) an introduction where you introduce the research question and hypothesis and briefly describe why it is interesting; (2) a data section that briefly describes the data source, describes how the key dependent and independent variables are measured (e.g., a survey, statistical model, or expert coding), and also produces a plot that summarizes the dependent variable; (3) a results section that contains a scatterplot, barplot, or boxplot of the main relationship of interest and output for the main regression of interest; and (4) a brief (one paragraph) concluding section that summarizes your results, assesses the extent to which you find support for your hypothesis, describes limitations of your analysis and threats to inference, and states how your analysis could be improved (e.g., improved data that would be useful to collect).\nFor the data section, you should note if your research design is cross-sectional (most projects will be of this type) or one of the other designs we discussed (randomized experiment, before-and-after, differences-in-differences). For the results section, you should interpret (in plain English) the main coefficient of interest in your regression. You should also comment on the statistical significance of the estimated coefficient and whether or not you believe the coefficient to represent a causal effect."
  },
  {
    "objectID": "assignments/index.html",
    "href": "assignments/index.html",
    "title": "Assignments",
    "section": "",
    "text": "Tutorial Instructions"
  },
  {
    "objectID": "assignments/index.html#problem-sets",
    "href": "assignments/index.html#problem-sets",
    "title": "Assignments",
    "section": "Problem Sets",
    "text": "Problem Sets\n\nProblem set 1: Data Visualization\nProblem set 2: Data Wrangling\nProblem set 3: Causality\nProblem set 4: Summarizing Data\nProblem set 5: Regression"
  },
  {
    "objectID": "assignments/index.html#final-project",
    "href": "assignments/index.html#final-project",
    "title": "Assignments",
    "section": "Final Project",
    "text": "Final Project\n\nFinal project information"
  },
  {
    "objectID": "assignments/problem-sets.html",
    "href": "assignments/problem-sets.html",
    "title": "Writing good codes",
    "section": "",
    "text": "For problem sets and final projects, I encourage you to write in Rmarkdown which will require two files:"
  },
  {
    "objectID": "assignments/problem-sets.html#writing-good-code",
    "href": "assignments/problem-sets.html#writing-good-code",
    "title": "Writing good codes",
    "section": "Writing good code",
    "text": "Writing good code\nYou’ll be writing code to do the analyses in this class. Code, like any language, provides many different ways of saying the same thing. One good practice of coding is to have what’s called good coding style. This refers to how you format the code that you so that it is (a) easy for you and others to read, and (b) less prone to making mistakes. Here are some general guidelines for writing R code and Rmd file.\n\nWhen writing R code and unless we tell you otherwise, follow the tidyverse style guide. For this class, the relevant parts of this document are the first couple of chapters. If you are ever uncertain about how to name something or how to write some code, see this document and it will likely help you quite a bit.\nMake sure the code chunks in the Rmd file have blank lines above and below them. If you don’t have this, it can sometimes cause problems with compilation.\nTry to keep your lines of code shorter than 80 characters since this makes reading code much easier. Usually this means writing some function arguments on a different line. See the style guide (section 2.5) for more on this.\nInclude comments in your code and format them nicely as in section 3.4 of the style guide. These comments should explain why you wrote the code you wrote and any notes you had about how you came to this solution. This might include, say, other approaches you tried but didn’t work or approaches you might want to try if you ever revisit this. Look at our code and examples to get a sense of how to use comments. In RStudio, you can nicely format a comment by hitting Control-Shift-\\."
  },
  {
    "objectID": "assignments/tutorials.html",
    "href": "assignments/tutorials.html",
    "title": "TPDstat R Tutorials Instructions",
    "section": "",
    "text": "In this class, we will be installing R tutorials locally to your RStudio. To install the package that contains the tutorials for the class run the following lines of code, each entered separately. Note that you may be asked to update packages when you enter this code. You can select 1 for “All” to perform any updates.\n\nremotes::install_github(\"kosukeimai/qss-package\", build_vignettes = TRUE)\nremotes::install_github(\"rstudio/learnr\")\nremotes::install_github(\"rstudio-education/gradethis\")\nremotes::install_github(\"seungho-an/TPDtutor\")\n\nAfter the package has been installed, you should be able to find all of the tutorials for the course in the “Tutorials” tab in the top-right pane of RStudio. Scroll down to find a TPDstat tutorial and click “Start Tutorial” to launch a tutorial:\n\nIf you do not see any QSS tutorials after scrolling to the bottom, try to restart RStudio, and check again. If you still don’t see any tutorials, confirm that you have installed the package by running library(TPDtutor). If you get an error about the package not being installed, try the above installation procedure again and note any error messages you receive before reaching out to the teaching staff for assistance."
  },
  {
    "objectID": "assignments/tutorials.html#faqs",
    "href": "assignments/tutorials.html#faqs",
    "title": "TPDstat R Tutorials Instructions",
    "section": "FAQs",
    "text": "FAQs\n\nWhat if I receive an error about pdflatex and the submission report doesn’t download?\nHere, you do not need to generate the report since there will be no grading in this class. Yet, if you still want to see the outcome, you need install LaTeX using the following command:\n\ninstall.packages('tinytex')\ntinytex::install_tinytex()  # install TinyTeX\n\n\n\nWhat if I don’t see a Tutorial tab in RStudio?\nMake sure that you have RStudio version 1.3 or higher installed on your computer. On a Mac, you can check the version by going to the top left of the menu bar (next to the Apple logo) and clicking on “RStudio” then “About RStudio”. On a Windows PC, you can find the same item under File > About.\n\n\nWhat if my submission report misses some of my attempts?\nThe submission report isn’t perfect and will sometimes say you didn’t attempt a question when you actually did. So, don’t worry about it."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TPDstat",
    "section": "",
    "text": "Introduction to Applied Statistics\n        \n        \n            Learning to use R to explore social, political and economic data\n        \n        \n            TPDStat • Spring 2023University of Arizona\n        \n    \n    \n      \n        \n        \n        \n      \n    \n\n\n\n\nInstructor\n\n   Prof. Seung-Ho An\n   SS 135\n   seunghoan@arizona.edu\n   seunghoan\n   Schedule an appointment\n\n\n\nCourse details\n\n   Wed.\n   March 1st-May 3rd, 2023\n   9:30–11:30 AM\n   TPD\n\n\n\nContacting me\nWe will have a group chat on “Teams.” If you have any questions, you can ask in the chat or send me an email. I normally try to answer within 24 hours. If you don’t hear from me in 48 hours, please feel free to send a reminder."
  },
  {
    "objectID": "materials/01_intro.html",
    "href": "materials/01_intro.html",
    "title": "Introduction to R and data visualization",
    "section": "",
    "text": "The syllabus and schedule pages.\nChapter 1 of Modern Dive\nCHapter 2 of Modern Dive"
  },
  {
    "objectID": "materials/01_intro.html#slides",
    "href": "materials/01_intro.html#slides",
    "title": "Introduction to R and data visualization",
    "section": "Slides",
    "text": "Slides\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)"
  },
  {
    "objectID": "materials/02_data_wrangling.html",
    "href": "materials/02_data_wrangling.html",
    "title": "Data Transformation and Wrangling",
    "section": "",
    "text": "Chapter 3 of Modern Dive"
  },
  {
    "objectID": "materials/02_data_wrangling.html#data",
    "href": "materials/02_data_wrangling.html#data",
    "title": "Data Transformation and Wrangling",
    "section": "Data",
    "text": "Data\n\nInstall the TPDdata package that has new data for this week:\n\n\nremotes::install_github(\"seungho-an/TPDdata\")"
  },
  {
    "objectID": "materials/02_data_wrangling.html#slides",
    "href": "materials/02_data_wrangling.html#slides",
    "title": "Data Transformation and Wrangling",
    "section": "Slides",
    "text": "Slides\n\nPDF slides\nPDF handout version (no incremental slides)\nCode: news.Rmd"
  },
  {
    "objectID": "materials/03_causality.html",
    "href": "materials/03_causality.html",
    "title": "Causality, randomized experiments, and observational studies",
    "section": "",
    "text": "Chapter 2 of QSS"
  },
  {
    "objectID": "materials/03_causality.html#data",
    "href": "materials/03_causality.html#data",
    "title": "Causality, randomized experiments, and observational studies",
    "section": "Data",
    "text": "Data\n\nReinstall the TPDstat data package if the current package does not have the dataset(s) from the lecture\n\n\nremotes::install_github(\"seungho-an/TPDdata\")"
  },
  {
    "objectID": "materials/03_causality.html#slides-and-code",
    "href": "materials/03_causality.html#slides-and-code",
    "title": "Causality, randomized experiments, and observational studies",
    "section": "Slides and Code",
    "text": "Slides and Code\n\nCausality and observational studies\n\nPDF slides\nPDF handout version (no incremental slides)\nCode: transphobia.Rmd"
  },
  {
    "objectID": "materials/04_measurement.html",
    "href": "materials/04_measurement.html",
    "title": "Summarizing Data and Survey Sampling",
    "section": "",
    "text": "Chapter 3 of QSS (though 3.6)"
  },
  {
    "objectID": "materials/04_measurement.html#data",
    "href": "materials/04_measurement.html#data",
    "title": "Summarizing Data and Survey Sampling",
    "section": "Data",
    "text": "Data\n\nReinstall the TPDstat data package if the current package does not have the dataset(s) from the lecture\n\n\nremotes::install_github(\"seungho-an/TPDdata\")"
  },
  {
    "objectID": "materials/04_measurement.html#slides-and-code",
    "href": "materials/04_measurement.html#slides-and-code",
    "title": "Summarizing Data and Survey Sampling",
    "section": "Slides and Code",
    "text": "Slides and Code\n\nSummarizing data and survey sampling\n\nPDF slides\nPDF handout version (no incremental slides)\nCode: cces.Rmd"
  },
  {
    "objectID": "materials/05_bivariate_tidying.html",
    "href": "materials/05_bivariate_tidying.html",
    "title": "Summarizing Relationships, Tidying and Joining Data",
    "section": "",
    "text": "For summarizing relationships, see QSS 3.6 and MD 5.1.1.\nFor pivoting and tidy data, see MD Ch 4.\nFor more on joins, see the chapter 20 in R for Data Science."
  },
  {
    "objectID": "materials/05_bivariate_tidying.html#data",
    "href": "materials/05_bivariate_tidying.html#data",
    "title": "Summarizing Relationships, Tidying and Joining Data",
    "section": "Data",
    "text": "Data\n\nReinstall the TPDstat data package if the current package does not have the dataset(s) from the lecture\n\n\nremotes::install_github(\"seungho-an/TPDdata\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will also use data from the nycflights13 package that you can install with:\n\n\ninstall.packages(\"nycflights13\")"
  },
  {
    "objectID": "materials/05_bivariate_tidying.html#slides-and-code",
    "href": "materials/05_bivariate_tidying.html#slides-and-code",
    "title": "Summarizing Relationships, Tidying and Joining Data",
    "section": "Slides and Code",
    "text": "Slides and Code\n\nSummarizing relationships, writing our own functions, causality review, pivoting longer, joining data\n\nPDF slides\nPDF handout version (no incremental slides)\nCode: covid_votes.Rmd\nCode: lecture-05.Rmd"
  },
  {
    "objectID": "materials/06_prediction.html",
    "href": "materials/06_prediction.html",
    "title": "Prediction: Elections and Regression",
    "section": "",
    "text": "For prediction and loops, see QSS 4.1 (you can skip 4.1.1)\nFor regression, see either MD Ch 5 or QSS 4.2 (you can skip 4.2.5)."
  },
  {
    "objectID": "materials/06_prediction.html#data",
    "href": "materials/06_prediction.html#data",
    "title": "Prediction: Elections and Regression",
    "section": "Data",
    "text": "Data\n\nReinstall the TPDdata package that has new data for this week:\n\n\nremotes::install_github(\"seungho-an/TPDdata\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou may need to install the lubridate and broom packages if it’s not installed:\n\n\ninstall.packages(\"lubridate\")\ninstall.packages(\"broom\")"
  },
  {
    "objectID": "materials/06_prediction.html#slides",
    "href": "materials/06_prediction.html#slides",
    "title": "Prediction: Elections and Regression",
    "section": "Slides",
    "text": "Slides\n\nPrediction, loops, and regression\n\nPDF slides\nPDF handout version (no incremental slides)"
  },
  {
    "objectID": "materials/07_regression.html",
    "href": "materials/07_regression.html",
    "title": "Prediction: More Regression",
    "section": "",
    "text": "For model fit, see QSS 4.2.6 or IMS 7.2.5\nFor multiple regression, IMS 8.1-8.3, MD 6.1-6.2, or QSS 4.3.1-4.3.2"
  },
  {
    "objectID": "materials/07_regression.html#data",
    "href": "materials/07_regression.html#data",
    "title": "Prediction: More Regression",
    "section": "Data",
    "text": "Data\n\nReinstall the TPDdata package that has new data for this week:\n\n\nremotes::install_github(\"seungho-an/TPDdata\")\n\n\nCSV files for the lab sessions:\n\ncct.csv\ntransphobia_all.csv"
  },
  {
    "objectID": "materials/07_regression.html#slides",
    "href": "materials/07_regression.html#slides",
    "title": "Prediction: More Regression",
    "section": "Slides",
    "text": "Slides\n\nModel fit and multiple regression\n\nPDF slides\nPDF handout version (no incremental slides)"
  },
  {
    "objectID": "materials/08_HE_regression.html",
    "href": "materials/08_HE_regression.html",
    "title": "Prediction: More Regression",
    "section": "",
    "text": "For model fit, see QSS 4.2.6 or IMS 7.2.5\nFor multiple regression, IMS 8.1-8.3, MD 6.1-6.2, or QSS 4.3.1-4.3.2"
  },
  {
    "objectID": "materials/08_HE_regression.html#data",
    "href": "materials/08_HE_regression.html#data",
    "title": "Prediction: More Regression",
    "section": "Data",
    "text": "Data\n\nReinstall the TPDdata package that has new data for this week:\n\n\nremotes::install_github(\"seungho-an/TPDdata\")\n\n\n\n\nData used during the class\n\ntransphobia_all.csv\nsocial.csv"
  },
  {
    "objectID": "materials/08_HE_regression.html#slides",
    "href": "materials/08_HE_regression.html#slides",
    "title": "Prediction: More Regression",
    "section": "Slides",
    "text": "Slides\n\nModel fit and multiple regression\n\nPDF slides\nPDF handout version (no incremental slides)"
  },
  {
    "objectID": "materials/09_sampling.html",
    "href": "materials/09_sampling.html",
    "title": "Sampling and Confidence Interval",
    "section": "",
    "text": "MD Ch 7 (sampling and sampling distribution)\nMD Ch 8, IMS Ch 12 for another take (confidence interval)"
  },
  {
    "objectID": "materials/09_sampling.html#data",
    "href": "materials/09_sampling.html#data",
    "title": "Sampling and Confidence Interval",
    "section": "Data",
    "text": "Data\n\nReinstall the TPDdata package that has new data for this week:\n\n\nremotes::install_github(\"seungho-an/TPDdata\")"
  },
  {
    "objectID": "materials/09_sampling.html#slides",
    "href": "materials/09_sampling.html#slides",
    "title": "Sampling and Confidence Interval",
    "section": "Slides",
    "text": "Slides\n\nSampling and bootstrapping\n\nPDF slides\nPDF handout version (no incremental slides)"
  },
  {
    "objectID": "materials/10_hyp_tests.html",
    "href": "materials/10_hyp_tests.html",
    "title": "Hypothesis Tests",
    "section": "",
    "text": "MD Ch 9, IMS Ch 11 for another take."
  },
  {
    "objectID": "materials/10_hyp_tests.html#data",
    "href": "materials/10_hyp_tests.html#data",
    "title": "Hypothesis Tests",
    "section": "Data",
    "text": "Data\n\nReinstall the TPDdata package that has new data for this week:\n\n\nremotes::install_github(\"seungho-an/TPDdata\")"
  },
  {
    "objectID": "materials/10_hyp_tests.html#slides-and-code",
    "href": "materials/10_hyp_tests.html#slides-and-code",
    "title": "Hypothesis Tests",
    "section": "Slides and Code",
    "text": "Slides and Code"
  },
  {
    "objectID": "materials/12_tests_models.html",
    "href": "materials/12_tests_models.html",
    "title": "More Hypothesis Tests and Mathematical Models",
    "section": "",
    "text": "IMS Ch 13 for Thursday."
  },
  {
    "objectID": "materials/12_tests_models.html#data",
    "href": "materials/12_tests_models.html#data",
    "title": "More Hypothesis Tests and Mathematical Models",
    "section": "Data",
    "text": "Data\n\nNo new data for Tuesday."
  },
  {
    "objectID": "materials/12_tests_models.html#slides-and-code",
    "href": "materials/12_tests_models.html#slides-and-code",
    "title": "More Hypothesis Tests and Mathematical Models",
    "section": "Slides and Code",
    "text": "Slides and Code\n\nTuesday (11/15) lecture: Difference in means tests and Power\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)\n\nThursday (11/17) lecture: Mathematical models for inference\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)"
  },
  {
    "objectID": "materials/13_models.html",
    "href": "materials/13_models.html",
    "title": "More Mathematical Models",
    "section": "",
    "text": "IMS Ch 15, 16."
  },
  {
    "objectID": "materials/13_models.html#slides-and-code",
    "href": "materials/13_models.html#slides-and-code",
    "title": "More Mathematical Models",
    "section": "Slides and Code",
    "text": "Slides and Code\n\nTuesday (11/22) lecture: Math models for hypothesis testing and difference in means\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)"
  },
  {
    "objectID": "materials/14_ols_inference.html",
    "href": "materials/14_ols_inference.html",
    "title": "The Bootstrap and Confidence Intervals",
    "section": "",
    "text": "QSS 7.3 and/or IMS Ch 24."
  },
  {
    "objectID": "materials/14_ols_inference.html#data",
    "href": "materials/14_ols_inference.html#data",
    "title": "The Bootstrap and Confidence Intervals",
    "section": "Data",
    "text": "Data\n\nReinstall the Gov 50 data package that has new data for this week:\n\n\nremotes::install_github(\"mattblackwell/gov50data\")\n\n\nCSV files for the data from this week if you cannot install the package:\n\najr.csv"
  },
  {
    "objectID": "materials/14_ols_inference.html#slides-and-code",
    "href": "materials/14_ols_inference.html#slides-and-code",
    "title": "The Bootstrap and Confidence Intervals",
    "section": "Slides and Code",
    "text": "Slides and Code\n\nTuesday (11/29) lecture: Inference with Linear Regression\n\nPDF of slides as I present them\nPDF of handout version of slides (no incremental slides)"
  },
  {
    "objectID": "materials/99_bootstrap.html",
    "href": "materials/99_bootstrap.html",
    "title": "The Bootstrap and Confidence Intervals",
    "section": "",
    "text": "MD Ch 8, IMS Ch 12 for another take."
  },
  {
    "objectID": "materials/99_bootstrap.html#data",
    "href": "materials/99_bootstrap.html#data",
    "title": "The Bootstrap and Confidence Intervals",
    "section": "Data",
    "text": "Data\n\nReinstall the TPDdata package that has new data for this week:\n\n\nremotes::install_github(\"seungho-an/TPDdata\")"
  },
  {
    "objectID": "materials/99_bootstrap.html#slides-and-code",
    "href": "materials/99_bootstrap.html#slides-and-code",
    "title": "The Bootstrap and Confidence Intervals",
    "section": "Slides and Code",
    "text": "Slides and Code"
  },
  {
    "objectID": "materials/index.html",
    "href": "materials/index.html",
    "title": "Readings and lectures",
    "section": "",
    "text": "Each class session has a set of required readings. Before attempting tutorials, you should complete the readings."
  },
  {
    "objectID": "resources/cheatsheet.html",
    "href": "resources/cheatsheet.html",
    "title": "TPDstat Cheat Sheet",
    "section": "",
    "text": "library(tidyverse)\nlibrary(gov50data)"
  },
  {
    "objectID": "resources/cheatsheet.html#r-basics-week-1",
    "href": "resources/cheatsheet.html#r-basics-week-1",
    "title": "TPDstat Cheat Sheet",
    "section": "R Basics (Week 1)",
    "text": "R Basics (Week 1)\n\nCreating a vector\nYou can create a vector using the c function:\n\n## Any R code that begins with the # character is a comment\n## Comments are ignored by R\n\nmy_numbers <- c(4, 8, 15, 16, 23, 42) # Anything after # is also a\n# comment\nmy_numbers\n\n[1]  4  8 15 16 23 42\n\n\n\n\nInstalling and loading a package\nYou can install a package with the install.packages function, passing the name of the package to be installed as a string (that is, in quotes):\n\ninstall.packages(\"ggplot2\")\n\nYou can load a package into the R environment by calling library() with the name of package without quotes. You should only have one package per library call.\n\nlibrary(ggplot2)\n\n\n\nCalling functions from specific packages\nWe can also use the mypackage:: prefix to access package functions without loading:\n\nknitr::kable(head(mtcars))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1"
  },
  {
    "objectID": "resources/cheatsheet.html#data-visualization-week-1",
    "href": "resources/cheatsheet.html#data-visualization-week-1",
    "title": "TPDstat Cheat Sheet",
    "section": "Data Visualization (week 1)",
    "text": "Data Visualization (week 1)\n\nScatter plot\nYou can produce a scatter plot with using the x and y aesthetics along with the geom_point() function.\n\nggplot(data = midwest,\n       mapping = aes(x = popdensity,\n                     y = percbelowpoverty)) +\n  geom_point()\n\n\n\n\n\n\nSmoothed curves\nYou can add a smoothed curve that summarizes the relationship between two variables with the geom_smooth() function. By default, it uses a loess smoother to estimated the conditional mean of the y-axis variable as a function of the x-axis variable.\n\nggplot(data = midwest,\n       mapping = aes(x = popdensity,\n                     y = percbelowpoverty)) +\n  geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\nAdding a regression line\ngeom_smooth can also add a regression line by setting the argument method = \"lm\" and we can turn off the shaded regions around the line with se = FALSE\n\nggplot(data = midwest,\n       mapping = aes(x = popdensity,\n                     y = percbelowpoverty)) +\n  geom_point() + geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nChanging the scale of the axes\nIf we want the scale of the x-axis to be logged to stretch out the data we can use the scale_x_log10():\n\nggplot(data = midwest,\n       mapping = aes(x = popdensity,\n                     y = percbelowpoverty)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_x_log10()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nAdding informative labels to a plot\nUse the labs() to add informative labels to the plot:\n\nggplot(data = midwest,\n       mapping = aes(x = popdensity,\n                     y = percbelowpoverty)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = FALSE) +\n  scale_x_log10() +\n  labs(x = \"Population Density\",\n       y = \"Percent of County Below Poverty Line\",\n       title = \"Poverty and Population Density\",\n       subtitle = \"Among Counties in the Midwest\",\n       source = \"US Census, 2000\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nMapping aesthetics to variables\nIf you would like to map an aesthetic to a variable for all geoms in the plot, you can put it in the aes call in the ggplot() function:\n\nggplot(data = midwest,\n       mapping = aes(x = popdensity,\n                     y = percbelowpoverty,\n                     color = state,\n                     fill = state)) +\n  geom_point() +\n  geom_smooth() +\n  scale_x_log10()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\nMapping aesthetics for a single geom\nYou can also map aesthetics for a specific geom using the mapping argument to that function:\n\nggplot(data = midwest,\n       mapping = aes(x = popdensity,\n                     y = percbelowpoverty)) +\n  geom_point(mapping = aes(color = state)) +\n  geom_smooth(color = \"black\") +\n  scale_x_log10()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\nSetting the aesthetics for all observations\nIf you would like to set the color or size or shape of a geom for all data points (that is, not mapped to any variables), be sure to set these outside of aes():\n\nggplot(data = midwest,\n       mapping = aes(x = popdensity,\n                     y = percbelowpoverty)) +\n  geom_point(color = \"purple\") +\n  geom_smooth() +\n  scale_x_log10()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\nHistograms\n\nggplot(data = midwest,\n       mapping = aes(x = percbelowpoverty)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "resources/index.html",
    "href": "resources/index.html",
    "title": "Resources",
    "section": "",
    "text": "R Tutorials Instructions (for running locally in RStudio)\nWriting good codes"
  },
  {
    "objectID": "resources/index.html#r-tutorials",
    "href": "resources/index.html#r-tutorials",
    "title": "Resources",
    "section": "R Tutorials",
    "text": "R Tutorials\n\nHands On Programming with R\nR for Data Science"
  },
  {
    "objectID": "resources/index.html#r-markdown",
    "href": "resources/index.html#r-markdown",
    "title": "Resources",
    "section": "R Markdown",
    "text": "R Markdown\n\nR Markdown Tutorial\nR Markdown: The Definitive Guide"
  },
  {
    "objectID": "resources/index.html#git-and-github-resources",
    "href": "resources/index.html#git-and-github-resources",
    "title": "Resources",
    "section": "Git and GitHub Resources",
    "text": "Git and GitHub Resources\n\nHappy Git with R"
  },
  {
    "objectID": "resources/office-hours.html",
    "href": "resources/office-hours.html",
    "title": "Office House Schedule",
    "section": "",
    "text": "TF/Instructor\n\n\nDay\n\n\nTime\n\n\nLocation\n\n\n\n\n\n\nDorothy\n\n\nMonday\n\n\n9:30-11:30AM\n\n\nCGIS Knafel Cafe (Fisher Family Commons)\n\n\n\n\nDan\n\n\nMonday\n\n\n10-11AM\n\n\nCGIS Knafel Cafe (Fisher Family Commons)\n\n\n\n\nAngelo\n\n\nTuesday\n\n\n1:30-3:30PM\n\n\nCGIS Knafel Cafe (Fisher Family Commons)\n\n\n\n\nMatt\n\n\nTuesday\n\n\n1:30-3:30PM\n\n\nCGIS K305 (Sign up form)\n\n\n\n\nSooahn\n\n\nWednesday\n\n\n9:30-11:30AM\n\n\nCGIS Knafel Cafe (Fisher Family Commons)\n\n\n\n\nDom\n\n\nWednesday\n\n\n10AM-12PM\n\n\nZoom (link on Ed)\n\n\n\n\nDan\n\n\nThursday\n\n\n3-4PM\n\n\nCGIS Knafel Cafe (Fisher Family Commons)\n\n\n\n\nMatt\n\n\nThursday\n\n\n1:30-3:30PM\n\n\nCGIS K305 (Sign up form)"
  },
  {
    "objectID": "resources/speaker-series.html",
    "href": "resources/speaker-series.html",
    "title": "Gov 50 Speaker Series",
    "section": "",
    "text": "David Sparks is the Director of Basketball Analytics at the Boston Celtics. This talk will be via Zoom."
  },
  {
    "objectID": "resources/speaker-series.html#october-21-3-4pm-asmae-toumi",
    "href": "resources/speaker-series.html#october-21-3-4pm-asmae-toumi",
    "title": "Gov 50 Speaker Series",
    "section": "October 21 (3-4pm): Asmae Toumi",
    "text": "October 21 (3-4pm): Asmae Toumi\nAsmae Toumi is the Director of Analytics at PursueCare, which is a telemedicine treatment company that offers virtual evidence-based addiction treatment for substance use disorders. This talk will be in person."
  },
  {
    "objectID": "resources/speaker-series.html#november-18-3-4pm-meg-schwenzfeier-cancelled",
    "href": "resources/speaker-series.html#november-18-3-4pm-meg-schwenzfeier-cancelled",
    "title": "Gov 50 Speaker Series",
    "section": "November 18 (3-4pm): Meg Schwenzfeier (cancelled)",
    "text": "November 18 (3-4pm): Meg Schwenzfeier (cancelled)\nMeg Schwenzfeier is currently the Data and Analytics Director for the Democratic Senatorial Campaign Committee. Previously she was the data science director at Biden for President, and worked as a data scientist at Hillary for America. This talk will be via Zoom."
  },
  {
    "objectID": "resources/speaker-series.html#december-2-3-4pm-solomon-messing-postponed-to-december-11",
    "href": "resources/speaker-series.html#december-2-3-4pm-solomon-messing-postponed-to-december-11",
    "title": "Gov 50 Speaker Series",
    "section": "December 2 (3-4pm): Solomon Messing (postponed to December 11)",
    "text": "December 2 (3-4pm): Solomon Messing (postponed to December 11)\nSolomon Messing is a Senior Engineering Manager Applied Sciences at Twitter and has previously held positions at Facebook, ACRONYM, and the Pew Research Center. This talk will be via Zoom."
  },
  {
    "objectID": "resources/study-halls.html",
    "href": "resources/study-halls.html",
    "title": "Study Hall Schedule",
    "section": "",
    "text": "CA Name\n\n\nDay\n\n\nTime\n\n\nLocation\n\n\n\n\n\n\nAngie\n\n\nSunday\n\n\n10am-12pm\n\n\nEliot Dining Hall\n\n\n\n\nOsvaldo\n\n\nSunday\n\n\n3-5pm\n\n\nAdams Dining Hall\n\n\n\n\nFelicia\n\n\nSunday\n\n\n8pm-10pm\n\n\nWinthrop Dining Hall\n\n\n\n\nDominic\n\n\nSunday\n\n\n8pm-10pm\n\n\nEliot Dining Hall\n\n\n\n\nAndy\n\n\nSunday\n\n\n10pm-12am\n\n\nWinthrop Beren Grill\n\n\n\n\nJason\n\n\nSunday\n\n\n8-10pm\n\n\nLeverett Dining Hall\n\n\n\n\nSarah\n\n\nMonday\n\n\n8-10am\n\n\nCabot Dining Hall\n\n\n\n\nReem\n\n\nMonday\n\n\n7-9pm\n\n\nLeveritt JCR\n\n\n\n\nAutumn\n\n\nMonday\n\n\n8-10pm\n\n\nWinthrop Dining Hall\n\n\n\n\nGroup Study Hall\n\n\nTuesday\n\n\n5:30-9:30pm\n\n\nCGIS Knafel Cafe (Fisher Family Commons)\n\n\n\n\n\n\nSchedule changes\n\nReem’s 10/3 study all is cancelled.\nAngie’s 10/9 study hall will be moved to 3-5pm on Zoom. See Slack for the link.\nAngie’s study hall on 10/30 will be moved to 11/1, 2-4pm in the Eliot Private Dining Room."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Below is the schedule for the semester. You can find the materials for each course meeting under the “Content” links for that week. You should generally:\n\ncomplete the readings by Tuesday;\ncomplete the tutorials by Tuesday (if any); and\ncomplete the problem set by Wednesday (before or after the class) (if any)\nProblem set answers will be posted either Thursday or Friday (if any)\n\nHere’s a guide to the schedule:\n\nMaterials (): This page contains the readings, slides, and recorded lectures (if any) for the topic. Read/watch these first.\nTutorial (): locally hosted tutorial.\nAssignment (): This page contains the instructions for each assignment.\n\nThe readings refer to following texts:\n\nQSS: Quantitative Social Science: An Introduction in tidyverse by Kosuke Imai and Nora Webb Williams\nMD: Statistical Inference via Data Science: A ModernDive into R and the Tidyverse by Chester Ismay and Albert Y. Kim\nIMS: Introduction to Modern Statistics by Mine Çetinkaya-Rundel and Johanna Hardin.\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading\n\n\nMaterials\n\n\nTutorial\n\n\nAssignment\n\n\n\n\n\n\nWeek 1\n\n\n\n\n3/1/23 \n\n\nIntroduction to the course\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3/1/23 \n\n\nR, Rstudio, and data visualization\n\n\nMD Ch 1-2\n\n\n\n\n\n\n\n\n\n\n\n\n\n3/1/23 \n\n\nTutorial 1 (R basic)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2\n\n\n\n\n3/8/23 \n\n\nTutorial 2 (Data wrangling)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3/8/23 \n\n\nData wrangling\n\n\nMD Ch 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n3/8/23 \n\n\nProblem Set 1 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3\n\n\n\n\n3/15/23\n\n\nTutorial 3 (Causality)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3/15/23\n\n\nCausal inference, randomized experiments, and observational studies\n\n\nQSS Ch 2.1-2.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n3/15/23\n\n\nProblem Set 2 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 4\n\n\n\n\n3/22/23\n\n\nTutorial 4 (Summarizing data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3/22/23\n\n\nSummarizing data & Survey sampling\n\n\nQSS Ch 2.6-3.4\n\n\n\n\n\n\n\n\n\n\n\n\n\n3/22/23\n\n\nProblem Set 3 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 5\n\n\n\n\n3/29/23\n\n\nTutorial 5 (Correlation and Tidying)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3/29/23\n\n\nSummarizing relationships in our data\n\n\nQSS Ch 3.5-3.6 & MD Ch4\n\n\n\n\n\n\n\n\n\n\n\n\n\n3/29/23\n\n\nData importing and tidy data\n\n\nMD Ch4\n\n\n\n\n\n\n\n\n\n\n\n\n\n3/29/23\n\n\nProblem Set 4 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 6\n\n\n\n\n4/5/23 \n\n\nPrediction & Iteration\n\n\nQSS 4.1 (except 4.1.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n4/5/23 \n\n\nTutorial 6 (Loops)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4/5/23 \n\n\nRegression\n\n\nMD Ch 5 or QSS 4.2 (except 4.2.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 7\n\n\n\n\n4/9/23 \n\n\nFinal Project Milestone 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4/26/23\n\n\nMultiple regression\n\n\nMD Ch 6.1-6.2 or QSS 4.3.1-4.3.2, 4.4.1\n\n\n\n\n\n\n\n\n\n\n\n\n\n4/26/23\n\n\nInterpreting regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4/26/23\n\n\nTutorial 7 (Regression)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 8\n\n\n\n\n4/30/23\n\n\nFinal Project Milestone 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5/3/23 \n\n\nHeterogenous effects in Regression\n\n\nQSS 4.4.2\n\n\n\n\n\n\n\n\n\n\n\n\n\n5/3/23 \n\n\nProblem Set 5 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 9\n\n\n\n\n5/10/23\n\n\nTutorial 8 (Bootstrapping)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5/10/23\n\n\nSampling and sampling distribution\n\n\nMD Ch 7\n\n\n\n\n\n\n\n\n\n\n\n\n\n5/10/23\n\n\nThe bootstrap and confidence intervals\n\n\nMD Ch 8/IMS Ch 12\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 10\n\n\n\n\n5/17/23\n\n\nHypothesis testing\n\n\nMD Ch 9/IMS Ch 11\n\n\n\n\n\n\n\n\n\n\n\n\n\n5/17/23\n\n\nProblem Set 6 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5/24/23\n\n\nFinal Project Milestone 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5/24/23\n\n\nProblem Set 7 due"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Prof. Seung-Ho An\n   SS 135\n   seunghoan@arizona.edu\n   seunghoan\n   Schedule an appointment\n\n\n\n\n\n   Wed.\n   March 1st-May 3rd, 2023\n   9:30–11:30 AM\n   TPD"
  },
  {
    "objectID": "syllabus.html#course-objectives",
    "href": "syllabus.html#course-objectives",
    "title": "Syllabus",
    "section": "Course objectives",
    "text": "Course objectives\nIn this course, you will learn the basics of data science as applied to the social sciences. This involves two broad skill sets: (1) learning the computing and programming tools to both manage and analyze data; and (2) understanding the conceptual foundations of why we might manage or analyze data in one way versus another. This course will address both of these topics.\nSpecifically, at the end of the course you should be able to:\n\nSummarize and visualize data\nWrangle messy data into tidy forms\nEvaluate claims about causality\nBe able to use linear regression to analyze data\nUnderstand uncertainty in data analysis and how to quantify it\nUse professional tools for data analysis such as R and RStudio\n\n\nExpectations\nIn this course, you will be expected to\n\ncomplete seven problem sets,\ncomplete eight tutorials, and\nwrite one final data analysis project (optional).\n\n\n\nPrerequisites\nNo prerequisites will be assumed. This course requires installing certain programs and R packages. If you are not familiar with these, feel free to reach out to me!"
  },
  {
    "objectID": "syllabus.html#course-structure",
    "href": "syllabus.html#course-structure",
    "title": "Syllabus",
    "section": "Course structure",
    "text": "Course structure\n\nFlow of the Course\nThe course will follow a basic flow each week, with small differences if a tutorial and/or problem set is provided.\n\nTuesday: Complete reading, complete tutorial (if due).\nWednesday: Lecture; complete problem set (if due)\n\n\n\nTutorials\nShort tutorials to assess your knowledge of the material covered in the reading materials that week will be provided.\n\n\nLectures\nWe will meet once a week for about 2 hours lecture where I will combine presenting material and may be doing live coding demonstrations. Ideally, you should bring your laptop to class and be ready to code along with me!\n\n\nGroup work\nEach individual should work on their own answer for problem sets and final project but working together as a group is highly encouraged. Even if you are already familiar with the topic, while helping others, you can internalize the tools and techniques that we cover in class. To solve programming issues (or social science issues in general), there is no single solution. Learning and solving problems together can broaden your skill sets and knowledge (this works really well!).\n\n\nProblem Sets\nStatistics and programming skills are not something that we can easily acquire by reading books. These skills are fairly similar to learning languages; if you don’t practice, you are likely to forget. In addition to basic tutorials, I will also provide problem sets which will give you an opportunity to apply the statistical techniques you are learning. They will usually be focused on data analysis in general and will often involve a real-world dataset. The answers will be provided after its due but your answers will not be graded."
  },
  {
    "objectID": "syllabus.html#final-project-optional",
    "href": "syllabus.html#final-project-optional",
    "title": "Syllabus",
    "section": "Final Project (optional)",
    "text": "Final Project (optional)\nThe final project is a data analysis project about whatever data excites you. This could be a project you are currently working on at TPD. If you want more challenges, I highly encourage you pursuing a project with your own dataset. Again, stats and programming can only be learned while doing it (and then repeat the processes over and over again). You are free to choose a topic. Yet, when you do so, you should formulate a key research question that includes a dependent variable(s) and independent variable(s). After that, you should find data that can answer the question and use the tools of the course to analyze the data. Also, keep in mind that when writing up the results, we should always target the general public as our audiance (not many people are as nerdy as we are). Graphs and tables should be interpreted with details!\nThe final project should include but not limited to (1) a brief introduction to the research question and data collected; (2) a visualization of the data in question that speaks to your research question; (3) a presentation (as a table or graph) of a regression model assessing your question along with a plain-English interpretation; (4) a brief (one-paragraph) section that describes limitations of your analysis and threats to inference, and states how your analysis could be improved (e.g., improved data that would be useful to collect).\n\n\n\nMilestone\nDue Date\n\n\n\n\nProposal\nSun, April 9nd\n\n\nDraft Analyses\nSun, April 16th\n\n\nFinal Report\nSun, May 7th\n\n\n\nNote that, the final project is optional, and due dates are tentative and can be quite flexible. The key purpose is for me to provide you feedback on any project that you may be pursuing. Once you complete the final report and want to keep pursuing the project, I will be happy to assist you as well."
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "Syllabus",
    "section": "Course Policies",
    "text": "Course Policies\n\nOffice Hours and Availability\nI am happy to meet during business hours (most cases from 10:00 to 3:00). If you have questions about the course material, computational issues, or other course-related issues please do not hesitate to set up an appointment with me. Since we all have a full-time job, I can meet during the weekends (if needed).\nIf you have a general question, I encourage posting it to the Teams chat since others are likely to have the same question. If you want to be anonymous, you can also email me directly at seunghoan@arizona.edu and I can post it to the Chat."
  },
  {
    "objectID": "syllabus.html#course-materials",
    "href": "syllabus.html#course-materials",
    "title": "Syllabus",
    "section": "Course materials",
    "text": "Course materials\n\nBooks\nWe will use the following books in this class:\n\nImai, Kosuke and Nora Webb Willaims. 2022. Quantitative Social Science: An Introduction with Tidyverse, 2022. Princeton University Press.\nIsmay, Chester and Albert Y. Kim. 2022. Statistical Inference via Data Science: A ModernDive into R and the Tidyverse.\nMine Cetinkaya-Rundel and Johanna Hardin. 2021. Introduction to Modern Statistics. OpenIntro.\n\nThe following books are optional, but may be helpful to build you understanding of the material:\n\nFreedman, David, Pisani, Robert, and Purves, Roger. 2007. Statistics. W.W. Norton & Company. 4th edition.\n\n\n\nComputing\nWe’ll use R in this class to conduct data analysis. R is free, open source, and available on all major platforms (including Solaris, so no excuses). RStudio (also free) is a graphical interface to R that is widely used to work with the R language. You can find a virtually endless set of resources for R and RStudio on the internet. For beginners, there are several web-based tutorials. In these, you will be able to learn the basic syntax of R."
  }
]